{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
      "          0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
      "          0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
      "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
      "          0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
      "          0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
      "          0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
      "          0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
      "          0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
      "          0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
      "          0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
      "          0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
      "          0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
      "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
      "          0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
      "          0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]]), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]))\n"
     ]
    }
   ],
   "source": [
    "# I personally don't like scatter, TODO make the one-hot encoder with another function\n",
    "mnist_data_train = datasets.MNIST(root=\"./data/\", download=True, train=True, transform=ToTensor(), target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1)))\n",
    "mnist_data_test = datasets.MNIST(root=\"./data/\", download=True,train=False, transform=ToTensor())\n",
    "data_loader_train = torch.utils.data.DataLoader(mnist_data_train, batch_size=16, shuffle=True)\n",
    "data_loader_test = torch.utils.data.DataLoader(mnist_data_test, batch_size=16, shuffle=True)\n",
    "print(mnist_data_train[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets & DataLoaders\n",
    "\n",
    "* Datasets and DataLoaders are a Pytorch's way to deal with data.\n",
    "* Dataset stores the data and it's properties(it must have a `__getitem__` and a `__len__` method) while DataLoader gives an iterator for a Dataset\n",
    "* In the DataLoader constructor, we can set some properties as the batch_size and shuffle.\n",
    "* The DataLoader will also allow us to use multiprocessing in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb44129e980>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using Dataset directly\n",
    "img, label = mnist_data_train[torch.randint(len(mnist_data_train)-1, size=(1,)).item()]\n",
    "plt.title(label=label)\n",
    "plt.imshow(img.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb4411b15a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using DataLoader\n",
    "\n",
    "features, labels = next(iter(data_loader_train))\n",
    "plt.title(label=(labels[0]==1).nonzero(as_tuple=True)[0].item())\n",
    "plt.imshow(features[0].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network class\n",
    "Every module is a subclass of nn.Module, every nn.Module subclass has to implement an `__init__()` and a `forward` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict: tensor([4])\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits= model(X)\n",
    "pred_prob = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_prob.argmax(1)\n",
    "print(f\"Predict: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer linear_relu_stack.0.weight - values: Parameter containing:\n",
      "tensor([[ 0.0330,  0.0333,  0.0098,  ..., -0.0195, -0.0350,  0.0309],\n",
      "        [ 0.0088,  0.0051, -0.0178,  ...,  0.0229,  0.0227,  0.0261],\n",
      "        [-0.0054, -0.0011, -0.0015,  ..., -0.0114,  0.0019,  0.0273],\n",
      "        ...,\n",
      "        [ 0.0278, -0.0301,  0.0329,  ..., -0.0277, -0.0213, -0.0212],\n",
      "        [ 0.0009,  0.0227, -0.0227,  ...,  0.0309,  0.0202,  0.0238],\n",
      "        [-0.0136, -0.0291,  0.0336,  ...,  0.0170,  0.0129,  0.0088]],\n",
      "       requires_grad=True)\n",
      "Layer linear_relu_stack.0.bias - values: Parameter containing:\n",
      "tensor([ 3.0652e-03,  1.2184e-02, -1.8588e-02, -7.3463e-03,  7.1099e-03,\n",
      "         1.2008e-02,  3.3164e-02,  3.5378e-02, -8.9719e-03,  1.6300e-02,\n",
      "        -1.6666e-02, -7.1762e-03, -2.2060e-03, -3.7772e-05,  3.2818e-02,\n",
      "        -1.6838e-02,  2.1002e-02,  2.6667e-03, -7.2948e-03, -3.2296e-02,\n",
      "         5.0562e-03,  1.8730e-02,  5.5449e-03, -1.9935e-02, -3.5239e-02,\n",
      "        -1.8136e-02, -1.6707e-02, -1.3659e-02, -2.4326e-02, -3.1130e-02,\n",
      "        -3.6618e-03, -1.4453e-02,  6.2764e-03, -2.0269e-02, -2.6598e-02,\n",
      "         2.0095e-02,  8.4875e-03, -2.3250e-02, -1.1636e-02, -3.4800e-02,\n",
      "        -3.5598e-02,  7.7978e-03, -3.4382e-02, -8.1236e-03,  2.1056e-02,\n",
      "         1.5970e-02,  2.7383e-02,  1.8022e-03, -1.2769e-03,  2.2493e-02,\n",
      "        -3.3908e-02,  3.1813e-02,  3.2759e-02,  3.4381e-02, -2.4853e-02,\n",
      "         2.8292e-02, -2.5769e-02,  7.2307e-03, -1.2995e-02, -3.6482e-03,\n",
      "        -2.4977e-02,  1.4712e-03,  2.9339e-02,  6.4874e-03, -5.4538e-06,\n",
      "        -1.0495e-02, -8.7046e-04,  5.3230e-03,  2.7168e-02,  1.3254e-02,\n",
      "        -4.6834e-03, -2.9826e-03,  1.4462e-02,  7.9041e-03, -6.1608e-03,\n",
      "         2.7265e-02,  2.2727e-02,  1.9860e-02, -5.3293e-03, -3.3894e-02,\n",
      "         2.8082e-02,  2.6869e-02, -5.9769e-03, -1.2188e-03,  3.1610e-03,\n",
      "         2.5658e-02, -2.4832e-02,  2.1535e-02, -3.4290e-02,  2.5680e-02,\n",
      "        -4.9636e-03,  1.7701e-02, -1.9703e-02,  3.2287e-02,  2.6464e-02,\n",
      "         2.9307e-02,  1.9901e-02,  2.0222e-02,  1.7450e-02, -2.0835e-03,\n",
      "        -1.1100e-02, -1.4911e-02, -2.7256e-02,  3.3223e-02,  2.3530e-02,\n",
      "        -3.7525e-03, -1.1208e-02, -2.7663e-02, -7.1310e-03,  2.0764e-03,\n",
      "         2.3951e-02,  3.5546e-02, -1.0101e-02,  1.3250e-02, -6.4294e-03,\n",
      "        -1.4007e-02,  3.5667e-02,  2.3047e-03,  6.6048e-03, -3.4479e-02,\n",
      "        -3.2143e-02, -2.2221e-02, -5.8969e-03,  1.9201e-02, -1.8363e-02,\n",
      "        -3.1217e-02, -1.8216e-02, -1.6282e-02, -2.5175e-02, -2.1604e-02,\n",
      "         2.9736e-02, -3.1959e-02, -2.9004e-02, -1.4636e-02, -1.7697e-02,\n",
      "        -3.3136e-02,  4.8249e-03,  2.9364e-02,  9.9679e-03, -2.5526e-02,\n",
      "        -1.7215e-02, -1.9593e-02,  1.7317e-02, -2.7818e-02,  2.2670e-02,\n",
      "        -4.4184e-03, -1.6974e-02, -2.8930e-02,  2.3288e-02,  2.1333e-02,\n",
      "        -1.2564e-02,  1.7617e-02,  3.1719e-03,  2.6584e-02,  1.9168e-02,\n",
      "         3.5511e-02, -1.6734e-02, -2.8824e-02,  1.6306e-02,  3.5287e-02,\n",
      "        -4.3722e-03,  1.3962e-02,  1.7233e-02, -5.8452e-03, -2.2335e-02,\n",
      "        -3.4570e-02,  3.5240e-02,  1.2158e-02, -2.0028e-02, -1.2547e-02,\n",
      "        -6.8362e-03,  1.0369e-02,  2.6446e-02, -4.5059e-03, -1.0028e-02,\n",
      "        -5.1737e-03, -2.3498e-02,  1.0469e-02,  3.2238e-02,  3.1889e-02,\n",
      "        -2.3191e-02, -3.0172e-02, -2.3530e-03, -2.9441e-02,  3.0400e-02,\n",
      "        -3.1665e-02, -3.4981e-02,  8.2533e-03, -5.5529e-03,  3.0952e-02,\n",
      "        -6.9592e-03,  2.4908e-02,  2.7333e-02,  1.9695e-02,  3.0104e-02,\n",
      "        -2.7239e-02,  5.8970e-03, -2.9590e-02, -2.5293e-03, -2.6863e-02,\n",
      "         2.5458e-02, -2.6995e-03, -2.2566e-02,  1.4293e-02, -2.6183e-03,\n",
      "         7.0042e-03, -1.2912e-02,  2.1893e-02, -5.1955e-03,  1.0857e-02,\n",
      "         5.4545e-03, -5.8501e-03,  1.4588e-03, -2.5634e-02,  2.2769e-02,\n",
      "         1.4749e-02,  3.2532e-02,  1.5262e-02,  8.6546e-03,  2.2212e-02,\n",
      "        -3.4789e-02,  3.4262e-02,  1.2442e-02,  6.5793e-03,  3.4284e-04,\n",
      "         2.0680e-02, -3.4417e-03,  1.1357e-02, -5.9503e-03,  1.7653e-02,\n",
      "        -5.0069e-03,  3.0291e-02,  3.2014e-02,  2.7270e-02,  2.1205e-02,\n",
      "        -3.3902e-02,  4.5634e-03, -1.6171e-02,  3.0997e-03,  1.8901e-02,\n",
      "        -6.3398e-03, -2.4480e-02,  3.2626e-02, -2.0753e-02, -1.6470e-02,\n",
      "        -2.6316e-02, -3.2770e-02,  3.5356e-02, -9.4100e-03, -2.2974e-02,\n",
      "         2.4081e-02,  3.4996e-02, -3.0213e-02, -4.2588e-03, -2.2018e-02,\n",
      "         2.5552e-02,  1.8159e-02,  4.0395e-03, -7.3005e-03, -3.2032e-02,\n",
      "        -1.8317e-02, -3.1756e-02, -1.8936e-02,  2.2162e-02, -1.1668e-02,\n",
      "         1.1618e-02, -3.4215e-02,  5.4794e-03,  1.1789e-02, -1.2935e-02,\n",
      "        -1.6528e-02, -3.2890e-03, -2.3117e-03,  1.6818e-02,  2.5536e-02,\n",
      "        -7.3190e-03,  1.5314e-02,  2.2342e-02,  1.2393e-02, -2.4709e-02,\n",
      "         3.4211e-02, -1.2685e-02, -2.7411e-02, -1.8338e-02,  2.1438e-02,\n",
      "        -2.9200e-02,  4.1285e-04, -2.2444e-02, -3.3259e-02, -3.2923e-02,\n",
      "         1.8467e-02, -1.8323e-02, -2.2730e-02,  2.4523e-02, -1.6880e-02,\n",
      "        -3.2399e-02,  3.1720e-02, -1.3950e-02, -2.7540e-02, -9.6475e-03,\n",
      "         9.8121e-03,  1.6455e-02,  2.3113e-03, -1.1498e-02,  1.4533e-02,\n",
      "        -1.9422e-02, -5.0657e-03,  1.9745e-02, -2.1241e-02,  2.7310e-02,\n",
      "         2.7088e-02, -8.1517e-03, -2.8987e-02,  1.9673e-02, -2.6357e-02,\n",
      "         3.1301e-02, -3.0163e-02,  2.7093e-02, -4.5313e-03,  2.2309e-02,\n",
      "         1.0765e-02, -1.5988e-02,  2.2618e-02,  2.6555e-02,  2.9882e-02,\n",
      "         2.2587e-02, -1.0441e-03, -4.8272e-03, -2.5906e-02,  2.0460e-02,\n",
      "        -6.0337e-04, -3.0336e-02, -2.1423e-02, -2.3514e-03, -2.6476e-02,\n",
      "        -1.2072e-02, -1.3147e-02, -3.2290e-02, -3.5058e-02,  5.4922e-03,\n",
      "         5.5501e-03,  1.9826e-02, -1.7963e-03, -9.2231e-03, -2.0423e-02,\n",
      "         7.9929e-03, -1.6347e-02, -1.8213e-02,  7.7864e-03,  8.8243e-03,\n",
      "        -3.6980e-05, -2.5638e-02, -3.1945e-02,  2.1419e-02, -1.4090e-02,\n",
      "         2.6928e-02, -2.6276e-02, -4.8403e-03, -1.2230e-03,  3.4888e-03,\n",
      "         1.1050e-02, -3.3491e-02,  1.6287e-02, -2.3319e-02, -1.4105e-02,\n",
      "         8.6838e-03, -1.0332e-02,  3.4715e-02, -1.9967e-02, -2.6854e-02,\n",
      "        -1.7069e-02, -3.1017e-02,  3.6485e-03,  2.0848e-02,  3.3466e-03,\n",
      "         1.2511e-02, -4.4262e-03, -1.5626e-03, -2.6531e-02, -1.0712e-02,\n",
      "        -1.6633e-02,  5.3207e-04,  9.5210e-03, -2.3900e-02,  2.5056e-02,\n",
      "        -2.0315e-02,  2.9155e-02,  1.6444e-02, -2.2926e-02, -9.9246e-04,\n",
      "        -2.2003e-02,  2.3970e-02,  3.2306e-03,  2.2279e-02,  2.7125e-02,\n",
      "         9.5481e-03, -3.4478e-02, -4.1728e-04, -3.5201e-02, -1.9598e-02,\n",
      "         2.5603e-02, -2.8468e-03, -1.0542e-02, -1.7672e-02,  7.6006e-03,\n",
      "        -7.8905e-03, -2.9660e-02, -1.3991e-02, -2.2464e-02, -1.0671e-02,\n",
      "        -1.5709e-02,  1.9996e-02,  2.4477e-02, -1.0424e-02,  1.5713e-03,\n",
      "        -2.4049e-02,  2.2078e-03,  9.0473e-03, -1.0138e-02, -3.5150e-03,\n",
      "        -1.2177e-02, -1.2388e-02,  3.0692e-02,  2.8380e-02,  2.8028e-02,\n",
      "        -2.5202e-02, -2.0302e-03, -5.9606e-03,  1.0161e-04, -3.4922e-03,\n",
      "         2.7650e-02,  2.5477e-02, -9.4409e-03,  1.2622e-02,  1.8049e-02,\n",
      "        -1.0543e-02, -2.5355e-02,  2.3989e-02,  8.7353e-03, -2.7469e-02,\n",
      "        -2.0253e-02,  3.2249e-02,  1.6022e-02, -2.9403e-04,  1.0904e-02,\n",
      "         2.7265e-02, -2.9386e-02, -1.6759e-02, -3.1540e-02,  1.2069e-02,\n",
      "        -5.8483e-03,  5.5847e-03,  3.1397e-03, -9.5677e-03, -2.9380e-03,\n",
      "        -2.9703e-02,  3.3129e-02,  8.4614e-03,  2.8459e-02,  3.0165e-02,\n",
      "        -6.1672e-03, -7.0087e-03,  2.4175e-02,  2.5139e-02, -3.3651e-02,\n",
      "         3.5695e-02,  1.4565e-02, -3.6200e-03, -3.2827e-02, -3.5450e-02,\n",
      "        -2.3532e-02, -2.2319e-02,  1.6791e-02, -1.1749e-02, -2.9535e-02,\n",
      "        -1.6055e-02, -8.2680e-03,  2.8392e-02, -1.4823e-02, -2.1122e-02,\n",
      "        -1.4751e-02,  9.0391e-03,  9.5985e-03, -3.1892e-02,  3.3695e-04,\n",
      "         2.8884e-02,  3.1660e-02, -1.4878e-02,  1.4573e-02, -1.2453e-02,\n",
      "         6.8222e-03, -1.4748e-02, -3.2977e-02, -1.8387e-02, -1.4696e-02,\n",
      "         2.2224e-02,  2.6362e-03, -1.3051e-02,  1.7104e-02,  1.4722e-02,\n",
      "         1.4529e-02,  2.6883e-02,  2.1117e-02, -2.5344e-02, -1.4334e-03,\n",
      "        -2.2251e-02,  1.2228e-02, -7.9053e-03,  2.2197e-02,  7.8538e-04,\n",
      "         1.1643e-02,  3.2813e-02], requires_grad=True)\n",
      "Layer linear_relu_stack.2.weight - values: Parameter containing:\n",
      "tensor([[-0.0280, -0.0056, -0.0219,  ...,  0.0386, -0.0429,  0.0035],\n",
      "        [ 0.0237, -0.0277, -0.0032,  ...,  0.0320, -0.0365, -0.0238],\n",
      "        [ 0.0308,  0.0382,  0.0194,  ..., -0.0117,  0.0311,  0.0058],\n",
      "        ...,\n",
      "        [ 0.0216,  0.0382, -0.0267,  ..., -0.0355, -0.0111,  0.0380],\n",
      "        [ 0.0400, -0.0188, -0.0257,  ..., -0.0210, -0.0129, -0.0373],\n",
      "        [ 0.0213,  0.0129, -0.0425,  ...,  0.0364,  0.0054, -0.0259]],\n",
      "       requires_grad=True)\n",
      "Layer linear_relu_stack.2.bias - values: Parameter containing:\n",
      "tensor([-0.0082, -0.0158, -0.0112, -0.0197, -0.0010, -0.0316, -0.0316, -0.0058,\n",
      "        -0.0280, -0.0082,  0.0027,  0.0339, -0.0424, -0.0118,  0.0095,  0.0237,\n",
      "         0.0189,  0.0169, -0.0249,  0.0365, -0.0361,  0.0404,  0.0088,  0.0156,\n",
      "        -0.0203, -0.0021,  0.0085, -0.0264,  0.0330, -0.0040, -0.0339,  0.0300,\n",
      "         0.0158,  0.0410,  0.0198,  0.0102, -0.0174,  0.0071, -0.0363, -0.0059,\n",
      "        -0.0354,  0.0307, -0.0408,  0.0411,  0.0146,  0.0412,  0.0047, -0.0375,\n",
      "        -0.0168, -0.0280,  0.0195, -0.0253,  0.0121,  0.0345, -0.0030, -0.0230,\n",
      "        -0.0382, -0.0400,  0.0137, -0.0080,  0.0278, -0.0327, -0.0143,  0.0096,\n",
      "        -0.0067, -0.0364, -0.0268, -0.0145, -0.0214,  0.0166,  0.0075, -0.0356,\n",
      "         0.0205, -0.0331,  0.0187, -0.0287,  0.0355,  0.0122,  0.0063, -0.0417,\n",
      "        -0.0401,  0.0031, -0.0175,  0.0383, -0.0307, -0.0411, -0.0172,  0.0296,\n",
      "         0.0432,  0.0384, -0.0383,  0.0013,  0.0439, -0.0063, -0.0328, -0.0243,\n",
      "        -0.0159,  0.0213, -0.0392, -0.0186, -0.0247,  0.0194,  0.0077,  0.0190,\n",
      "        -0.0249, -0.0436,  0.0317, -0.0184,  0.0417, -0.0019, -0.0052,  0.0400,\n",
      "        -0.0238,  0.0052, -0.0315, -0.0323,  0.0351,  0.0003, -0.0061,  0.0206,\n",
      "        -0.0181,  0.0323, -0.0313, -0.0108, -0.0271,  0.0343, -0.0415, -0.0436,\n",
      "         0.0387,  0.0099,  0.0358,  0.0441, -0.0294,  0.0093, -0.0124, -0.0245,\n",
      "         0.0064,  0.0323, -0.0017,  0.0338, -0.0270,  0.0134, -0.0122,  0.0127,\n",
      "        -0.0199,  0.0137,  0.0331,  0.0220,  0.0293,  0.0376,  0.0083,  0.0339,\n",
      "        -0.0393, -0.0329, -0.0402, -0.0365,  0.0198,  0.0303,  0.0009, -0.0042,\n",
      "         0.0270, -0.0332, -0.0146, -0.0423,  0.0156,  0.0116, -0.0211,  0.0028,\n",
      "         0.0065,  0.0072,  0.0395,  0.0165,  0.0293, -0.0038, -0.0366,  0.0216,\n",
      "        -0.0120,  0.0230, -0.0406,  0.0415, -0.0149, -0.0208,  0.0079, -0.0021,\n",
      "         0.0422,  0.0197, -0.0072, -0.0347,  0.0287, -0.0230,  0.0371,  0.0206,\n",
      "        -0.0173, -0.0081,  0.0404, -0.0290, -0.0057, -0.0127, -0.0089,  0.0173,\n",
      "         0.0026,  0.0070,  0.0085,  0.0266, -0.0061,  0.0022, -0.0310,  0.0307,\n",
      "        -0.0378,  0.0193, -0.0022, -0.0135, -0.0016,  0.0251, -0.0211,  0.0156,\n",
      "        -0.0424, -0.0230, -0.0171,  0.0191,  0.0373,  0.0159,  0.0029,  0.0110,\n",
      "        -0.0243,  0.0403,  0.0422,  0.0243, -0.0441, -0.0374, -0.0431,  0.0351,\n",
      "         0.0156,  0.0226,  0.0439,  0.0262, -0.0315,  0.0265,  0.0250, -0.0044,\n",
      "         0.0372, -0.0259, -0.0081,  0.0090,  0.0023,  0.0212,  0.0219,  0.0302,\n",
      "         0.0392, -0.0148,  0.0329,  0.0202, -0.0392,  0.0405,  0.0215, -0.0184,\n",
      "         0.0253, -0.0244,  0.0228, -0.0405,  0.0386,  0.0293, -0.0040, -0.0238,\n",
      "         0.0012, -0.0371, -0.0254,  0.0394,  0.0310, -0.0438, -0.0115,  0.0179,\n",
      "         0.0217,  0.0236,  0.0396,  0.0204, -0.0432,  0.0300, -0.0372,  0.0242,\n",
      "         0.0340,  0.0127,  0.0384,  0.0040,  0.0141,  0.0204,  0.0374, -0.0314,\n",
      "         0.0048,  0.0029, -0.0370,  0.0213,  0.0006, -0.0009, -0.0439,  0.0223,\n",
      "         0.0041, -0.0296,  0.0187,  0.0122, -0.0089,  0.0060, -0.0314,  0.0034,\n",
      "         0.0193,  0.0061,  0.0434, -0.0306, -0.0152, -0.0283,  0.0114,  0.0345,\n",
      "        -0.0198,  0.0276, -0.0131,  0.0159,  0.0081, -0.0033, -0.0391,  0.0256,\n",
      "        -0.0248,  0.0321,  0.0177,  0.0188,  0.0011, -0.0120, -0.0327,  0.0296,\n",
      "        -0.0293, -0.0120, -0.0286, -0.0405, -0.0428, -0.0281, -0.0442, -0.0171,\n",
      "         0.0134,  0.0314, -0.0123, -0.0383,  0.0036,  0.0049,  0.0037, -0.0343,\n",
      "         0.0144, -0.0393,  0.0030, -0.0097,  0.0153, -0.0345, -0.0422, -0.0011,\n",
      "         0.0065, -0.0372, -0.0255,  0.0273, -0.0392,  0.0122, -0.0310,  0.0411,\n",
      "         0.0197, -0.0322,  0.0124,  0.0011,  0.0363, -0.0141, -0.0018,  0.0136,\n",
      "        -0.0053, -0.0278,  0.0426, -0.0097,  0.0216, -0.0399,  0.0413,  0.0187,\n",
      "        -0.0274,  0.0074,  0.0077, -0.0329, -0.0111, -0.0328,  0.0182,  0.0284,\n",
      "        -0.0009,  0.0416,  0.0370,  0.0008,  0.0122,  0.0319, -0.0230, -0.0225,\n",
      "         0.0372,  0.0347,  0.0110, -0.0301,  0.0305, -0.0280,  0.0331,  0.0309,\n",
      "         0.0253, -0.0341, -0.0102, -0.0028, -0.0367,  0.0084,  0.0152,  0.0328,\n",
      "        -0.0277,  0.0380, -0.0209, -0.0123, -0.0115, -0.0069, -0.0254,  0.0229,\n",
      "         0.0076,  0.0373,  0.0297, -0.0160,  0.0378,  0.0013, -0.0320, -0.0115,\n",
      "        -0.0058,  0.0128,  0.0003, -0.0439, -0.0369, -0.0238,  0.0062, -0.0428,\n",
      "        -0.0384,  0.0028,  0.0132, -0.0057,  0.0248, -0.0344, -0.0150, -0.0060,\n",
      "         0.0142, -0.0380, -0.0397, -0.0420,  0.0188, -0.0256, -0.0165,  0.0264,\n",
      "         0.0008,  0.0251,  0.0393, -0.0135,  0.0026, -0.0373, -0.0095,  0.0236,\n",
      "         0.0339,  0.0393, -0.0358, -0.0006,  0.0027, -0.0233,  0.0189,  0.0156,\n",
      "        -0.0119, -0.0096, -0.0060, -0.0217, -0.0151,  0.0245,  0.0350,  0.0406,\n",
      "        -0.0227, -0.0056, -0.0204,  0.0406, -0.0366, -0.0419,  0.0414,  0.0122,\n",
      "        -0.0144,  0.0365,  0.0069,  0.0221,  0.0308, -0.0214,  0.0368,  0.0341,\n",
      "         0.0060,  0.0264,  0.0005,  0.0015, -0.0297, -0.0154, -0.0311, -0.0293,\n",
      "        -0.0156, -0.0133,  0.0248, -0.0022,  0.0406, -0.0169, -0.0333,  0.0340,\n",
      "        -0.0301, -0.0300,  0.0259,  0.0302, -0.0276,  0.0005, -0.0273,  0.0323],\n",
      "       requires_grad=True)\n",
      "Layer linear_relu_stack.4.weight - values: Parameter containing:\n",
      "tensor([[-0.0375, -0.0182, -0.0327,  ..., -0.0149, -0.0239, -0.0100],\n",
      "        [-0.0019, -0.0179,  0.0204,  ...,  0.0239, -0.0117, -0.0055],\n",
      "        [ 0.0430,  0.0121,  0.0217,  ..., -0.0070,  0.0387, -0.0432],\n",
      "        ...,\n",
      "        [ 0.0187, -0.0320, -0.0216,  ..., -0.0161, -0.0427,  0.0400],\n",
      "        [ 0.0313, -0.0017, -0.0345,  ..., -0.0416, -0.0072, -0.0371],\n",
      "        [-0.0269, -0.0119, -0.0297,  ..., -0.0256,  0.0024,  0.0256]],\n",
      "       requires_grad=True)\n",
      "Layer linear_relu_stack.4.bias - values: Parameter containing:\n",
      "tensor([ 0.0163, -0.0154,  0.0152, -0.0083, -0.0083,  0.0330,  0.0383,  0.0232,\n",
      "        -0.0193,  0.0277], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer {name} - values: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for classification\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "Optization happens in three steps in the training loop:\n",
    "* reset the gradients by calling `optimizer.zero_grad()`\n",
    "* backpropagate the prediction loss using `loss.backward()`. The gradient of the loss for each parameter will be stored\n",
    "* adjust the parameters using the gradients collected in backpropagation step using `optimizer.step()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(data_loader, model, loss_fn, optimizer):\n",
    "    size = len(data_loader.dataset)\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch+1)*len(X) # each batch will have len(X) samples\n",
    "            print(f\"loss: {loss} [{current}/{size}]\")\n",
    "\n",
    "def test_loop(data_loader, model, loss_fn):\n",
    "    size = len(data_loader.dataset)\n",
    "    num_batches = len(data_loader)\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad(): # makes requires_grad = False for all nodes of the computational graph\n",
    "        for X, y in data_loader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Accuracy {100*correct:0.1f}%, average loss: {test_loss:0.8f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      " --------------------\n",
      "tensor([[-2.4523e-03, -1.2558e-01,  2.7437e-01, -2.5401e-01, -3.7736e-02,\n",
      "         -2.2795e-01,  5.6953e-01, -1.5014e-01,  1.2078e-01, -5.0079e-02],\n",
      "        [ 2.8910e-01, -3.0291e-01, -6.6864e-02,  4.8068e-01, -1.2203e-01,\n",
      "          1.8545e-01,  1.2600e-02, -1.9507e-01,  1.2068e-01, -9.5227e-02],\n",
      "        [-1.3768e-01,  1.3323e-01,  5.1605e-01,  2.5938e-01, -2.8309e-01,\n",
      "         -2.8359e-01,  1.6355e-01, -1.0040e-01,  2.4755e-01, -2.5322e-01],\n",
      "        [ 7.1515e-01, -3.7319e-01,  1.1346e-01,  6.6947e-02, -1.0258e-01,\n",
      "          9.5811e-02,  1.5946e-01, -2.2176e-01, -2.4146e-02, -2.2528e-01],\n",
      "        [-1.5742e-01, -6.1493e-02, -3.6646e-02,  2.9116e-02,  1.3314e-01,\n",
      "         -1.0427e-01, -2.4814e-02,  3.6589e-01,  1.7350e-02,  2.9001e-01],\n",
      "        [-2.4348e-03, -6.8924e-02,  3.9137e-02,  9.5336e-02,  6.2789e-02,\n",
      "         -5.9914e-02,  3.2530e-02,  1.9594e-02,  7.6804e-02,  1.2594e-01],\n",
      "        [ 2.3750e-01, -5.1471e-01,  3.4948e-01, -3.7357e-01,  3.5801e-01,\n",
      "         -3.9105e-01,  6.3517e-01,  3.4706e-02, -9.2797e-02,  1.3636e-01],\n",
      "        [ 6.1673e-01, -2.7605e-01,  1.9443e-02,  9.8531e-02, -5.9911e-02,\n",
      "          1.0172e-01,  4.8596e-02, -1.2336e-01, -6.6002e-02, -1.0942e-01],\n",
      "        [ 6.3317e-02, -5.9690e-02,  7.3324e-02,  4.5424e-01, -2.1067e-01,\n",
      "         -3.3732e-02,  1.5865e-01, -1.9344e-01,  1.8748e-01, -1.0934e-01],\n",
      "        [-9.7066e-02, -1.9082e-01,  3.5171e-01,  1.7176e-01, -5.8197e-02,\n",
      "         -1.7321e-01,  2.5821e-01, -2.9048e-01,  2.5370e-01, -1.0288e-01],\n",
      "        [-1.8012e-01, -6.3894e-02,  4.1369e-01, -1.0166e-01,  7.3951e-02,\n",
      "         -3.5536e-01,  2.0569e-01,  4.4105e-04,  5.1695e-02,  3.8999e-02],\n",
      "        [-3.6437e-01,  6.2950e-01,  1.9583e-01,  1.3451e-01, -3.3008e-01,\n",
      "         -2.6513e-01, -1.9001e-02, -1.0027e-01,  2.3230e-01, -2.2317e-01],\n",
      "        [-2.6305e-01,  5.4393e-01,  2.0794e-01,  1.4460e-01, -3.7127e-01,\n",
      "         -2.3371e-01,  2.0128e-02, -1.3453e-01,  2.5639e-01, -2.5581e-01],\n",
      "        [-2.3763e-01, -2.5736e-01, -1.4573e-01, -3.0460e-01,  5.1585e-01,\n",
      "         -3.4462e-01, -1.2794e-02,  5.9702e-01,  7.5098e-02,  7.3597e-01],\n",
      "        [ 5.1713e-01, -5.1228e-01, -6.0767e-02,  5.7686e-02,  4.2690e-02,\n",
      "          1.1055e-01,  9.2226e-02, -1.3101e-01,  3.0771e-01,  1.6144e-01],\n",
      "        [ 2.8019e-01, -1.6810e-01,  6.7015e-02,  3.8832e-01, -2.5813e-01,\n",
      "          6.0988e-02, -4.5998e-02, -3.0631e-01,  2.4977e-01, -1.5767e-01]]) tensor([6, 3, 2, 0, 7, 9, 6, 0, 3, 2, 2, 1, 1, 9, 0, 3]) tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.])\n",
      "<class 'torch.Tensor'> tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m --------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# train_loop(data_loader_train, model, loss_fn, optimizer)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m test_loop(data_loader_test, model, loss_fn)\n",
      "Cell \u001b[0;32mIn[32], line 30\u001b[0m, in \u001b[0;36mtest_loop\u001b[0;34m(data_loader, model, loss_fn)\u001b[0m\n\u001b[1;32m     28\u001b[0m test_loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m num_batches\n\u001b[1;32m     29\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(correct), correct)\n\u001b[0;32m---> 30\u001b[0m correct \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m size\n\u001b[1;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAccuracy \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m100\u001b[39m\u001b[39m*\u001b[39mcorrect\u001b[39m:\u001b[39;00m\u001b[39m0.1f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%, average loss: \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m:\u001b[39;00m\u001b[39m0.8f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: result type Float can't be cast to the desired output type Long"
     ]
    }
   ],
   "source": [
    "epochs = 3 \n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1} \\n --------------------\")\n",
    "    train_loop(data_loader_train, model, loss_fn, optimizer)\n",
    "    test_loop(data_loader_test, model, loss_fn)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
